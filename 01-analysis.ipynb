{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from src.util import prnt\n",
    "from src.util import print_all_output\n",
    "from src.settings import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.load import Load\n",
    "semester = 'WS2025_26' # WS2024_25 WS2023_24',   'dump20240826'), 'SS2025' \n",
    "# Step 1\n",
    "l = Load(semester=semester, dump=\"dump20251115\")\n",
    "#l.process_textedits(filter_weeks=1)\n",
    "#\n",
    "l.run(filter_weeks=1)\n",
    "df = l.get_data()\n",
    "#df\n",
    "# load all data of a group\n",
    "#l.run(filter_group=[2751],filter_weeks=True)\n",
    "df[df['moodle_group_id'] == 5083]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: EasySync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pipx install numpy>=1.24.0 scikit-learn>=1.3.0\n",
    "!pipx install textstat\n",
    "!pipx install language_tool_python\n",
    "!pipx install textdescriptives\n",
    "!pipx install matplotlib\n",
    "#!spacy download de_core_news_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.extract_easy_sync import Extract_Easy_Sync\n",
    "from src.text_quality import Preprocess_Text_Quality\n",
    "semester = 'WS2025_26'\n",
    "period_split_interval='weeks'\n",
    "# Step 2\n",
    "#print_all_output = False\n",
    "ptq = Preprocess_Text_Quality(semester)\n",
    "sft = semester_form_to_arr[semester]  # variable in settings.py\n",
    "time_breaks = ptq.split_text_progression_by_threshold(\n",
    "    df,\n",
    "    semester, \n",
    "    start_date=datetime(sft[0][0], sft[0][1], sft[0][2]), \n",
    "    end_date=datetime(sft[1][0], sft[1][1], sft[1][2]),\n",
    "    group_column='moodle_pad_id',\n",
    "    period_split_interval=period_split_interval\n",
    "    )\n",
    "time_breaks.sort_values(by=['moodle_group_id', 'moodle_pad_id'])\n",
    "\n",
    "es = Extract_Easy_Sync(semester, time_breaks)\n",
    "es.is_reconstructing_text = False\n",
    "df_textchanges = es.extract_easy_sync(df)\n",
    "df_textchanges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [experimental] Step 2.1: Extract sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.extract_sessions import Extract_Sessions\n",
    "\n",
    "semester = 'SS2024'\n",
    "es = Extract_Sessions(semester)\n",
    "#dd = es.load_and_combine_df() # TODO: All authorids in the pad_commit file are derived from moodle but not from etherpad\n",
    "#dd['authorid'].unique()\n",
    "#dd[dd['authorid'].isnull()]\n",
    "df_textchanges.columns\n",
    "#df_textchanges['moodle_user_id'] = df_textchanges['moodle_user_id'].astype('int')\n",
    "\n",
    "df_textchanges['moodle_group_id'] = df_textchanges['moodle_group_id'].astype('int')\n",
    "df_sessions = es.extract_sessions(df_textchanges)\n",
    "df_sessions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  [experimental] Step 2.2: Extract and Analyze Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\"\"\"\n",
    "This script extracts all texts per group/pad and defined periods.\n",
    "The resulting text versions are stored as files. Then the text is analyzed for certain features.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from src.util import prnt\n",
    "from src.util import print_all_output\n",
    "from src.settings import *\n",
    "from src.text_quality import Preprocess_Text_Quality\n",
    "from src.extract_easy_sync import Extract_Easy_Sync\n",
    "\n",
    "semester = 'SS2025'\n",
    "period_split_interval='weeks'\n",
    "\n",
    "# Step 1: identify time breaks\n",
    "ptq = Preprocess_Text_Quality(semester)\n",
    "sft = semester_form_to_arr[semester]  # variable in settings.py\n",
    "time_breaks = ptq.split_text_progression_by_threshold(\n",
    "    #df[df['moodle_group_id']==3925], \n",
    "    df,\n",
    "    semester, \n",
    "    start_date=datetime(sft[0][0], sft[0][1], sft[0][2]), \n",
    "    end_date=datetime(sft[1][0], sft[1][1], sft[1][2]),\n",
    "    group_column='moodle_pad_id',\n",
    "    period_split_interval=period_split_interval\n",
    "    )\n",
    "time_breaks.sort_values(by=['moodle_group_id', 'moodle_pad_id'])\n",
    "\n",
    "\n",
    "es = Extract_Easy_Sync(semester, time_breaks=time_breaks, period_split_interval=period_split_interval)\n",
    "es.is_reconstructing_text = True\n",
    "print_all_output = True\n",
    "\n",
    "##pads = df[df['moodle_pad_id']==df['moodle_pad_id'].unique()[14]].groupby('moodle_pad_id', group_keys=True)\n",
    "pads = df.groupby('moodle_pad_id', group_keys=True)\n",
    "\n",
    "i = 0\n",
    "for index, pad in pads:\n",
    "    print(' ')\n",
    "    for row in pad.sort_values(by=['timestamp']).itertuples():\n",
    "        i = i + 1\n",
    "        if i > 0:\n",
    "            es.extract_changeset(\n",
    "                row[5],             # row['textedit_changeset'], \n",
    "                timestamp=row[6],   # row['timestamp'], \n",
    "                group_id=row[3],    # row['moodle_group_id'], \n",
    "                pad_id=row[4],      # str(row['moodle_pad_id'])\n",
    "            )\n",
    "    #print(es.ttext)\n",
    "\n",
    "ptq = Preprocess_Text_Quality(semester)\n",
    "res_tq = ptq.determine_text_quality_from_files(period_split_interval=period_split_interval)\n",
    "res_tq\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  [experimental] nlp test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.text_quality import Preprocess_Text_Quality, Text_Quality\n",
    "from src.extract_easy_sync import Extract_Easy_Sync\n",
    "import de_core_news_sm\n",
    "\n",
    "text = \"Der Mann geht Joggen.\"\n",
    "tq = Text_Quality()\n",
    "tq.run(text, model='de_core_news_lg') # 'de_core_news_md'\n",
    "\n",
    "import de_core_news_sm\n",
    "\n",
    "nlp = de_core_news_sm.load()\n",
    "\n",
    "tokenized = nlp(text)\n",
    "for token in tokenized:\n",
    "    print(token, token.pos_, token.ent_type_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 : Extract_Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from src.extract_neighbours import Extract_Neighbours\n",
    "\n",
    "# Step 3\n",
    "print_all_output=False\n",
    "semester = 'SS2025'\n",
    "en = Extract_Neighbours(semester)\n",
    "author_relations = en.extract_neighbours(df_textchanges)\n",
    "author_relations\n",
    "\n",
    "#author_relations[author_relations['right_neighbor']>0]\n",
    "#df_textchanges['moodle_pad_id_'] = df_textchanges['moodle_pad_id'].str.split('$', expand=True)[0]\n",
    "#pads = df_textchanges['moodle_pad_id_'].unique()\n",
    "#df_textchanges[df_textchanges['moodle_pad_id_'] == pads[3]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Extract_Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.extract_degrees import Extract_Degree\n",
    "\n",
    "# Step 4\n",
    "semester = 'SS2025'\n",
    "ed = Extract_Degree(semester)\n",
    "author_relations_summary = ed.summarize_individual_level(author_relations)\n",
    "\n",
    "#author_relations_summary\n",
    "#author_relations_summary['right2'] = author_relations_summary['right'].astype('int').abs() \n",
    "#author_relations_summary['right2']\n",
    "#author_relations_summary.dtypes\n",
    "\n",
    "print_all_output = False #True\n",
    "author_degrees = ed.extract_degree(author_relations_summary)\n",
    "# TODO: During the processing the size of the dataframe varys pretty much. Something is wrong with the agg function. Check if this behavior is ok.\n",
    "#print(author_relations_summary.size)\n",
    "#print(author_degrees.size)\n",
    "#author_degrees\n",
    "\n",
    "print_all_output = True\n",
    "author_degrees_per_group = ed.map_to_group(df_textchanges, author_degrees)\n",
    "#print('df_textchanges', df_textchanges)\n",
    "#print('author_degrees', author_degrees)\n",
    "#author_degrees_per_group[author_degrees_per_group['indegree_count']>0]\n",
    "author_degrees_per_group\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Coll Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.collaboration_graph import Collaboration_Graph\n",
    "\n",
    "semester = 'WS2025_26'\n",
    "cg = Collaboration_Graph(semester)\n",
    "#cg.check_random_group(author_relations)\n",
    "graph_measures = cg.create_graph_for_all_groups(author_relations, save_plot=False, save_output=True, show_plot=False)\n",
    "graph_measures\n",
    "cg.create_json_graph_for_all_groups(\n",
    "    author_relations, \n",
    "    last_modified = 1763370511517,\n",
    "    save_to_file=True,\n",
    "    target_week=\"week-2\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peer Review Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from src.extract_peer_review import extract_peer_review\n",
    "\n",
    "extract_peer_review(input_dir = './data/peer-review-dumps', output_dir = './output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All at once: Group behavior in all semesters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pipx install textstat==0.7.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Kernel: Python 3.12.5\n",
    "\n",
    "\"\"\"Step 0: Run over all semesters\"\"\"\n",
    "import gc  # Import garbage collector\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from src.util import prnt\n",
    "from src.util import print_all_output\n",
    "from src.settings import *\n",
    "from src.load import Load\n",
    "from src.extract_easy_sync import Extract_Easy_Sync\n",
    "from src.extract_sessions import Extract_Sessions\n",
    "from src.extract_neighbours import Extract_Neighbours\n",
    "from src.extract_degrees import Extract_Degree\n",
    "from src.collaboration_graph import Collaboration_Graph\n",
    "from src.text_quality import Preprocess_Text_Quality\n",
    "\n",
    "semesters = [\n",
    "    # not available # 'SS2021', \n",
    "    #('WS2021_22', 'dump20240826'),\n",
    "    #('SS2022',    'dump20240826'),\n",
    "    #('WS2022_23', 'dump20240826'),\n",
    "    #('SS2023',    'dump20240826'),\n",
    "    #('WS2023_24',   'dump20240826'),\n",
    "    #('SS2024',    'dump20240826'),\n",
    "    #('WS2024_25',  'dump20241209'),\n",
    "    ('SS2025',   'dump20250520'),\n",
    "    #('WS2025_26',  'dump20251215'),\n",
    "]\n",
    "project_name = 'edm26' # e.g., lak25\n",
    "period_split_interval = 'weeks'\n",
    "save_csv_output = True\n",
    "print_all_output = False\n",
    "compute_texts = False\n",
    "for the_semester in semesters:\n",
    "    print('# Process semester '+ the_semester[0])\n",
    "    semester = the_semester[0]\n",
    "    the_dump = the_semester[1]\n",
    "    period_1 = get_period(semester, period_1_arr)\n",
    "    period_2 = get_period(semester, period_2_arr)\n",
    "    period_3 = get_period(semester, period_3_arr)\n",
    "    # print('  Periods: ', period_1, period_2, period_3)\n",
    "    print('## step1 - Load data ('+semester+')')\n",
    "    l = Load(semester, the_dump)\n",
    "    l.run(filter_weeks=1)\n",
    "    #l.process_textedits(filter_weeks=1)\n",
    "    df = l.get_data()\n",
    "    \n",
    "    # Step 2\n",
    "    # Step 2.1: identify time breaks\n",
    "    ptq = Preprocess_Text_Quality(semester)\n",
    "    sft = semester_form_to_arr[semester]  # variable in settings.py\n",
    "    time_breaks = ptq.split_text_progression_by_threshold(\n",
    "        df,\n",
    "        semester, \n",
    "        start_date=datetime(sft[0][0], sft[0][1], sft[0][2]), \n",
    "        end_date=datetime(sft[1][0], sft[1][1], sft[1][2]),\n",
    "        group_column='moodle_pad_id',\n",
    "        #group_column='moodle_group_id',\n",
    "        period_split_interval=period_split_interval\n",
    "        )\n",
    "    time_breaks.sort_values(by=['moodle_group_id', 'moodle_pad_id'])\n",
    "    \n",
    "    print('## step2 - Extract_Easy_Sync ('+semester+')')\n",
    "    es = Extract_Easy_Sync(semester, time_breaks)\n",
    "    es.is_reconstructing_text = compute_texts\n",
    "    df_textchanges = es.extract_easy_sync(df)\n",
    "    del df  # Free memory\n",
    "    gc.collect()  # Force garbage collection\n",
    "    \n",
    "    print('## step3 Extract Sessions (skipped) ('+semester+')') # TODO\n",
    "    ##es = Extract_Sessions(semester)\n",
    "    ##df_sessions = es.extract_sessions(df_textchanges)\n",
    "    \n",
    "    # Step 4\n",
    "    print('## step4 - Extract Text Versions (optional) ('+semester+')')\n",
    "    if compute_texts == True:\n",
    "        all_text = pd.read_csv(f'{output_path}{project_name}-{semester}-02.1-text-revisions.csv')\n",
    "        ptq = Preprocess_Text_Quality(semester)\n",
    "        #sft = semester_form_to_arr[semester]  # variable in settings.py\n",
    "        df_raw_text = ptq.split_text_progression_by_threshold(\n",
    "            all_text, \n",
    "            semester, \n",
    "            datetime(sft[0][0], sft[0][1], sft[0][2]), \n",
    "            datetime(sft[1][0], sft[1][1], sft[1][2])\n",
    "            )\n",
    "        res = ptq.determine_text_quality(df_raw_text) # will be saved to file 2.2\n",
    "    \n",
    "    \n",
    "    en = Extract_Neighbours(semester, period_split_interval, save_output=save_csv_output)\n",
    "    \n",
    "    ed = Extract_Degree(semester, period_split_interval, save_output=save_csv_output)\n",
    "    df_textchanges['timestamp'] = df_textchanges['timestamp'].astype(float)\n",
    "    date_thresholds = es.generate_observation_times(\n",
    "        datetime(sft[0][0], sft[0][1], sft[0][2]), \n",
    "        datetime(sft[1][0], sft[1][1], sft[1][2]),\n",
    "        period_split_interval=period_split_interval\n",
    "        )\n",
    "    date_thresholds = [1765752289451] # hotfix\n",
    "    print(date_thresholds)\n",
    "    for subset_until in date_thresholds:\n",
    "        print(f'## process period {list(date_thresholds).index(subset_until)+1}/{len(date_thresholds)}')\n",
    "        df_textchanges_until_break = df_textchanges[df_textchanges['timestamp'] < subset_until]\n",
    "        if df_textchanges_until_break.empty:\n",
    "            print('arg xxx')\n",
    "            continue\n",
    "        \n",
    "        print('### step5.1 - Extract_Neighbours ('+semester+')')\n",
    "        author_relations = en.extract_neighbours(df_textchanges_until_break, subset_until) # before: df_textchanges xxx\n",
    "        \n",
    "        print('### step5.2 - Extract Degree ('+semester+')')\n",
    "        author_relations_summary = ed.summarize_individual_level(author_relations, subset_until)\n",
    "        author_degrees = ed.extract_degree(author_relations_summary, subset_until)\n",
    "        author_degrees_per_group = ed.map_to_group(df_textchanges_until_break, author_degrees, subset_until)  \n",
    "        del df_textchanges_until_break, author_degrees_per_group, author_relations_summary\n",
    "        gc.collect()\n",
    "        \n",
    "        print('### step6 - Collaboration_Graph ('+semester+')')\n",
    "        cg = Collaboration_Graph(semester, period_split_interval, save_output=save_csv_output)\n",
    "        # TODO: save_plot imply displaying the plot\n",
    "        graph_measures = cg.create_graph_for_all_groups(author_relations, subset_until, save_plot=False, save_output=True, show_plot=False)\n",
    "        \n",
    "        #graph_measures\n",
    "        cg.create_json_graph_for_all_groups(\n",
    "            author_relations, \n",
    "            last_modified = 1765752289451,\n",
    "            save_to_file=True,\n",
    "            target_week=\"week-6\"\n",
    "            )\n",
    "        del author_relations, author_degrees, cg, graph_measures\n",
    "        gc.collect()\n",
    "    # fin\n",
    "\n",
    "    del df_textchanges\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [experimental] All at once: Text features for all text versions in all semesters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\"\"\"\n",
    "This script iterates over all semesters and loads the textedit data from csv and extracts the EasySync data. \n",
    "During the extraction versions of the text documents are reconstructed for provided periods (e.g. per day or week). \n",
    "The resulting text version is stored as txt file in the output folder\n",
    "\"\"\"\n",
    "\n",
    "import gc  # Import garbage collector\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from src.util import prnt\n",
    "from src.util import print_all_output\n",
    "from src.settings import *\n",
    "from src.load import Load\n",
    "from src.extract_easy_sync import Extract_Easy_Sync\n",
    "from src.text_quality import Preprocess_Text_Quality\n",
    "\n",
    "semesters = [\n",
    "    # not available # 'SS2021', \n",
    "    ##('WS2021_22', 'dump20240826'),\n",
    "    ##('SS2022',    'dump20240826'),\n",
    "    ##('WS2022_23', 'dump20240826'),\n",
    "    ##('SS2023',    'dump20240826'),\n",
    "    ##('WS2023_24',   'dump20240826'),\n",
    "    ##('SS2024',    'dump20240826'),\n",
    "    ('WS2024_25',  'dump20241209'),\n",
    "]\n",
    "period_split_interval = 'weeks'\n",
    "print_all_output = False\n",
    "compute_texts = False\n",
    "for the_semester in semesters:\n",
    "    print('# Process semester '+ the_semester[0])\n",
    "    semester = the_semester[0]\n",
    "    the_dump = the_semester[1]\n",
    "    period_1 = get_period(semester, period_1_arr)\n",
    "    period_2 = get_period(semester, period_2_arr)\n",
    "    period_3 = get_period(semester, period_3_arr)\n",
    "    print('  Periods: ', period_1, period_2, period_3)\n",
    "    print('## step1 - Load data')\n",
    "    l = Load(semester, the_dump)\n",
    "    #l.run()\n",
    "    l.process_textedits(filter_weeks=1)\n",
    "    df = l.get_data()\n",
    "\n",
    "    print(f'## step2 - identify time breaks in semester {the_semester[0]}') \n",
    "    ptq = Preprocess_Text_Quality(semester)\n",
    "    sft = semester_form_to_arr[semester]  # variable in settings.py\n",
    "    time_breaks = ptq.split_text_progression_by_threshold(\n",
    "        df,\n",
    "        semester, \n",
    "        start_date=datetime(sft[0][0], sft[0][1], sft[0][2]), \n",
    "        end_date=datetime(sft[1][0], sft[1][1], sft[1][2]),\n",
    "        group_column='moodle_pad_id',\n",
    "        period_split_interval=period_split_interval\n",
    "        )\n",
    "    time_breaks.sort_values(by=['moodle_group_id', 'moodle_pad_id'])\n",
    "\n",
    "    print(f'## step3 - extract text versions in semester {the_semester[0]}') \n",
    "    es = Extract_Easy_Sync(semester, time_breaks=time_breaks, period_split_interval=period_split_interval)\n",
    "    es.is_reconstructing_text = True\n",
    "    print_all_output = True\n",
    "    pads = df.groupby('moodle_pad_id', group_keys=True)\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    i = 0\n",
    "    for index, pad in pads:\n",
    "        print(' ')\n",
    "        for row in pad.sort_values(by=['timestamp']).itertuples():\n",
    "            i = i + 1\n",
    "            if i > 0:\n",
    "                es.extract_changeset(\n",
    "                    row[5],             # row['textedit_changeset'], \n",
    "                    timestamp=row[6],   # row['timestamp'], \n",
    "                    group_id=row[3],    # row['moodle_group_id'], \n",
    "                    pad_id=row[4],      # str(row['moodle_pad_id'])\n",
    "                )\n",
    "        #print(es.ttext)\n",
    "    del pads, es\n",
    "    gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\"\"\"\n",
    "Compute text features from all text versions\n",
    "\"\"\"\n",
    "import gc  # Import garbage collector\n",
    "from src.settings import *\n",
    "from src.text_quality import Preprocess_Text_Quality\n",
    "\n",
    "semesters = [\n",
    "    # not available # 'SS2021', \n",
    "    ##('WS2021_22', 'dump20240826'),\n",
    "    ('SS2022',    'dump20240826'),\n",
    "    ##('WS2022_23', 'dump20240826'),\n",
    "    ##('SS2023',    'dump20240826'),\n",
    "    ##('WS2023_24',   'dump20240826'),\n",
    "    ##('SS2024',    'dump20240826'),\n",
    "    ##('WS2024_25',  'dump20241209'),\n",
    "]\n",
    "period_split_interval = 'weeks'\n",
    "min_index = 3500\n",
    "max_index = 5000\n",
    "for the_semester in semesters:\n",
    "    semester = the_semester[0]\n",
    "    ptq = Preprocess_Text_Quality(semester)    \n",
    "    print('## step4 - Compute text features') \n",
    "    prefix = 'lak25-' + semester\n",
    "    res_tq = ptq.determine_text_quality_from_files(\n",
    "        period_split_interval=period_split_interval, \n",
    "        prefix = prefix,\n",
    "        min_index = min_index,\n",
    "        max_index = max_index\n",
    "        )\n",
    "    del ptq.tq, ptq\n",
    "    gc.collect()\n",
    "res_tq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Define paths\n",
    "the_paths = [\n",
    "    #'datasets/CSCL01-WS2019_20-M1-DivAdapt',\n",
    "    #'datasets/CSCL02-SS2020-M1-DivAdapt'\n",
    "    #'datasets/CSCL03-WS2020_21-M1-DivAdapt',\n",
    "    #'datasets/CSCL04-SS2021-M1-DivAdapt',\n",
    "    'datasets/CSCL05-WS2021_22-M1-DivAdapt',\n",
    "    'datasets/CSCL06-SS2022-M1-DivAdapt',\n",
    "    'datasets/CSCL07-WS2022_23-M1-DivAdapt',\n",
    "    'datasets/CSCL08-SS2023-M1-DivAdapt',\n",
    "    'datasets/CSCL09-WS2023_24-M1-MULTIDIVERSE-CSCL',\n",
    "    'datasets/CSCL10-SS2024-M1-MULTIDIVERSE-CSCL',\n",
    "    'datasets/CSCL11-WS2024_25-M1-MULTIDIVERSE-CSCL',\n",
    "    'datasets/CSCL12-SS2025-M1-MULTIDIVERSE-CSCL'\n",
    "]\n",
    "\n",
    "# Process group data\n",
    "df_list = []\n",
    "for i, path in enumerate(the_paths):\n",
    "    # Extract semester using regex\n",
    "    semester_match = re.search(r\"(SS|WS)\\d{4}(_\\d{2})?\", path)\n",
    "    semester = semester_match.group(0) if semester_match else \"\"\n",
    "    \n",
    "    print(f'Load data of {semester}')\n",
    "    filepath = f'../../{path}/data/{semester}-etherpad-07-group-graph-measures-groups.csv'\n",
    "    \n",
    "    # Read CSV file\n",
    "    tmp = pd.read_csv(filepath)\n",
    "    \n",
    "    # Convert 'until' to datetime and calculate weeks\n",
    "    tmp['until'] = pd.to_datetime(tmp['until'])\n",
    "    min_until = tmp['until'].min()\n",
    "    tmp['week'] = ((tmp['until'] - min_until).dt.days // 7) + 1\n",
    "    \n",
    "    # Filter and select columns\n",
    "    tmp = (tmp\n",
    "           .query('week < 7')\n",
    "           .query('activeUsers > 1')\n",
    "           .drop(columns=['until']))\n",
    "    \n",
    "    df_list.append(tmp)\n",
    "\n",
    "# Combine all dataframes\n",
    "df_groups = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Remove specific columns\n",
    "columns_to_drop = ['density2', 'degree_centrality', 'closeness_centrality']\n",
    "df_groups = df_groups.drop(columns=[col for col in columns_to_drop if col in df_groups.columns])\n",
    "\n",
    "print(\"df_groups shape:\", df_groups.shape)\n",
    "print(\"df_groups columns:\", df_groups.columns.tolist())\n",
    "\n",
    "# Process member data\n",
    "df_list_members = []\n",
    "for i, path in enumerate(the_paths):\n",
    "    # Extract semester using regex\n",
    "    semester_match = re.search(r\"(SS|WS)\\d{4}(_\\d{2})?\", path)\n",
    "    semester = semester_match.group(0) if semester_match else \"\"\n",
    "    \n",
    "    print(f'Load data of {semester}')\n",
    "    filepath = f'../../{path}/data/{semester}-etherpad-06-author-degrees.csv'\n",
    "    \n",
    "    # Read CSV file\n",
    "    tmp = pd.read_csv(filepath)\n",
    "    \n",
    "    # Convert 'until' to datetime and calculate weeks\n",
    "    tmp['until'] = pd.to_datetime(tmp['until'])\n",
    "    min_until = tmp['until'].min()\n",
    "    tmp['week'] = ((tmp['until'] - min_until).dt.days // 7) + 1\n",
    "    \n",
    "    # Filter by week\n",
    "    tmp = tmp.query('week < 7')\n",
    "    \n",
    "    # Group by moodle_group_id and week, then summarize\n",
    "    tmp = (tmp\n",
    "           .groupby(['moodle_group_id', 'week'])\n",
    "           .agg({\n",
    "               'moodle_user_id': 'count',  # group_size = n()\n",
    "               'outdegree_chars': 'sum',\n",
    "               'selfdegree_chars': 'sum', \n",
    "               'selfdegree_count': 'std'\n",
    "           })\n",
    "           .reset_index())\n",
    "    \n",
    "    # Rename and calculate columns\n",
    "    tmp = tmp.rename(columns={\n",
    "        'moodle_user_id': 'group_size',\n",
    "        'selfdegree_count': 'selfdegree_sd'\n",
    "    })\n",
    "    \n",
    "    # Calculate total_chars\n",
    "    tmp['total_chars'] = tmp['outdegree_chars'] + tmp['selfdegree_chars']\n",
    "    \n",
    "    # Drop the individual char columns and keep only what we need\n",
    "    tmp = tmp.drop(columns=['outdegree_chars', 'selfdegree_chars'])\n",
    "    \n",
    "    # Sort and add semester\n",
    "    tmp = tmp.sort_values(['moodle_group_id', 'week'])\n",
    "    tmp['semester'] = semester\n",
    "    \n",
    "    df_list_members.append(tmp)\n",
    "\n",
    "# Combine all member dataframes\n",
    "df_members = pd.concat(df_list_members, ignore_index=True)\n",
    "\n",
    "print(\"df_members shape:\", df_members.shape)\n",
    "print(\"df_members columns:\", df_members.columns.tolist())\n",
    "\n",
    "# Join the dataframes (right join)\n",
    "df = df_groups.merge(\n",
    "    df_members.drop(columns=['semester', 'group_size']), \n",
    "    on=['moodle_group_id', 'week'], \n",
    "    how='right'\n",
    ")\n",
    "\n",
    "# Relocate semester column after week\n",
    "cols = df.columns.tolist()\n",
    "if 'semester' in cols:\n",
    "    cols.remove('semester')\n",
    "    week_idx = cols.index('week')\n",
    "    cols.insert(week_idx + 1, 'semester')\n",
    "    df = df[cols]\n",
    "\n",
    "print(\"Final df shape:\", df.shape)\n",
    "print(\"Final df columns:\", df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep tslearn data\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tslearn.utils import to_time_series_dataset\n",
    "\n",
    "# Step 1: Load the CSV file\n",
    "csv_path = \"/Users/nise/Documents/proj_001_doc/pub/110-2025-LAK-CollWriting/cscl-writing/data/projects/lak2026/output/df-cluster.csv\"  # Replace with your actual file path\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Step 2: Define the feature columns (all except identifiers)\n",
    "feature_columns = [\n",
    "    \"activeUsers\", \"isolatedUsers\", \"usersProvideLotHelp\", \n",
    "    \"usersProvideAvgHelp\", \"usersProvideLittleHelp\", \"usersProvideNoHelp\",\n",
    "    \"usersReceiveLotHelp\", \"usersReceiveAvgHelp\", \"usersReceiveLittleHelp\",\n",
    "    \"usersReceiveNoHelp\", \"closeCouples\", \"avgCouples\", \"loseCouples\",\n",
    "    \"density1\", \n",
    "    #\"total_chars\", \n",
    "    \"selfdegree_sd\"\n",
    "]\n",
    "\n",
    "# Step 3: Get unique group IDs\n",
    "unique_groups = df['moodle_group_id'].unique()\n",
    "print(f\"Found {len(unique_groups)} unique moodle groups\")\n",
    "\n",
    "# Step 4: Create time series for each group\n",
    "time_series_list = []\n",
    "group_ids = []\n",
    "\n",
    "for group_id in unique_groups:\n",
    "    # Filter data for this group\n",
    "    group_data = df[df['moodle_group_id'] == group_id].copy()\n",
    "    \n",
    "    # Sort by week to ensure proper time ordering\n",
    "    group_data = group_data.sort_values('week')\n",
    "    \n",
    "    # Extract feature values for all weeks\n",
    "    # Each row becomes a time point, each column becomes a feature dimension\n",
    "    group_time_series = group_data[feature_columns].values\n",
    "    \n",
    "    # Only add groups that have data\n",
    "    if len(group_time_series) > 0:\n",
    "        time_series_list.append(group_time_series)\n",
    "        group_ids.append(group_id)\n",
    "\n",
    "# Step 5: Convert to tslearn format\n",
    "formatted_dataset = to_time_series_dataset(time_series_list)\n",
    "formatted_dataset = np.nan_to_num(formatted_dataset, nan=0.0)\n",
    "\n",
    "print(f\"Group IDs created: {len(group_ids)} groups\")\n",
    "\n",
    "# Step 6: Create labels (choose one of the options below)\n",
    "\n",
    "# Option 1: Create labels based on a column in your data (e.g., semester)\n",
    "# Get the semester for each group (using the first occurrence for each group)\n",
    "y = []\n",
    "for group_id in group_ids:\n",
    "    group_semester = df[df['moodle_group_id'] == group_id]['week'].iloc[0]\n",
    "    y.append(group_semester)\n",
    "y = np.array(y)\n",
    "\n",
    "\n",
    "# Step 7: Display information about the dataset\n",
    "print(f\"Dataset shape: {formatted_dataset.shape}\")\n",
    "print(f\"Number of time series: {formatted_dataset.shape[0]}\")\n",
    "print(f\"Number of time points per series: {formatted_dataset.shape[1]}\")\n",
    "print(f\"Number of features per time point: {formatted_dataset.shape[2]}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"Unique labels: {np.unique(y)}\")\n",
    "\n",
    "# Step 8: Now you can use with tslearn\n",
    "from tslearn.neighbors import KNeighborsTimeSeriesClassifier\n",
    "knn = KNeighborsTimeSeriesClassifier(n_neighbors=5)\n",
    "knn.fit(formatted_dataset, y)\n",
    "\n",
    "from tslearn.svm import TimeSeriesSVC\n",
    "#clf = TimeSeriesSVC(C=1.0, kernel=\"gak\")\n",
    "#clf.fit(formatted_dataset, y)\n",
    "\n",
    "from tslearn.shapelets import LearningShapelets\n",
    "#clf = LearningShapelets(n_shapelets_per_size={4: 1})\n",
    "#clf.fit(formatted_dataset, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tslearn timeseries clustering .. multivariate\n",
    "\n",
    "from tslearn.clustering import TimeSeriesKMeans, KernelKMeans\n",
    "\n",
    "# Method 1: Elbow method to find optimal number of clusters\n",
    "def find_optimal_clusters_elbow(data, max_k=20):\n",
    "    inertias = []\n",
    "    K_range = range(2, max_k + 1)\n",
    "    \n",
    "    for k in K_range:\n",
    "        kmeans = TimeSeriesKMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(data)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    # Plot elbow curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(K_range, inertias, 'bo-')\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.title('Elbow Method for Optimal k')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return K_range, inertias\n",
    "\n",
    "# Method 2: Silhouette analysis\n",
    "def find_optimal_clusters_silhouette(data, max_k=20):\n",
    "    silhouette_scores = []\n",
    "    K_range = range(2, max_k + 1)\n",
    "    \n",
    "    for k in K_range:\n",
    "        kmeans = TimeSeriesKMeans(n_clusters=k, random_state=42)\n",
    "        cluster_labels = kmeans.fit_predict(data)\n",
    "        \n",
    "        # Flatten the time series for silhouette calculation\n",
    "        data_flat = data.reshape(data.shape[0], -1)\n",
    "        silhouette_avg = silhouette_score(data_flat, cluster_labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "        print(f\"k={k}: Silhouette Score = {silhouette_avg:.3f}\")\n",
    "    \n",
    "    # Plot silhouette scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(K_range, silhouette_scores, 'ro-')\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('Average Silhouette Score')\n",
    "    plt.title('Silhouette Analysis for Optimal k')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    optimal_k = K_range[np.argmax(silhouette_scores)]\n",
    "    print(f\"Optimal number of clusters: {optimal_k}\")\n",
    "    return optimal_k, silhouette_scores\n",
    "\n",
    "# Run clustering analysis\n",
    "print(\"Finding optimal number of clusters...\")\n",
    "optimal_k, silhouette_scores = find_optimal_clusters_silhouette(formatted_dataset, max_k=20)\n",
    "\n",
    "# Perform clustering with optimal k\n",
    "print(f\"\\nPerforming clustering with k={optimal_k}\")\n",
    "kmeans_optimal = TimeSeriesKMeans(n_clusters=optimal_k, random_state=42)\n",
    "cluster_labels = kmeans_optimal.fit_predict(formatted_dataset)\n",
    "\n",
    "print(f\"Cluster distribution:\")\n",
    "unique, counts = np.unique(cluster_labels, return_counts=True)\n",
    "for cluster, count in zip(unique, counts):\n",
    "    print(f\"  Cluster {cluster}: {count} time series\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYTHON APPROACHES FOR MULTIVARIATE TIME SERIES CLUSTERING\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from tslearn.clustering import TimeSeriesKMeans, KernelKMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from dtaidistance import dtw\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate sample multivariate time series data\n",
    "np.random.seed(42)\n",
    "n_series = 100\n",
    "n_timepoints = 50\n",
    "n_variables = 3\n",
    "\n",
    "# Create synthetic data with different patterns\n",
    "data = []\n",
    "for i in range(n_series):\n",
    "    if i < 33:  # Cluster 1: trending up\n",
    "        ts = np.random.randn(n_timepoints, n_variables) + np.linspace(0, 2, n_timepoints)[:, np.newaxis]\n",
    "    elif i < 66:  # Cluster 2: oscillating\n",
    "        t = np.linspace(0, 4*np.pi, n_timepoints)\n",
    "        ts = np.random.randn(n_timepoints, n_variables) + np.column_stack([np.sin(t), np.cos(t), np.sin(2*t)])\n",
    "    else:  # Cluster 3: stationary\n",
    "        ts = np.random.randn(n_timepoints, n_variables)\n",
    "    \n",
    "    data.append(ts)\n",
    "\n",
    "data = np.array(data)\n",
    "print(f\"Data shape: {data.shape} (n_series, n_timepoints, n_variables)\")\n",
    "\n",
    "# METHOD 1: DTW-based clustering with tslearn\n",
    "print(\"\\n=== METHOD 1: DTW K-Means ===\")\n",
    "\n",
    "# Normalize the time series\n",
    "scaler = TimeSeriesScalerMeanVariance()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# DTW K-means clustering\n",
    "dtw_kmeans = TimeSeriesKMeans(n_clusters=3, metric=\"dtw\", random_state=42)\n",
    "dtw_labels = dtw_kmeans.fit_predict(data_scaled)\n",
    "\n",
    "print(f\"DTW K-means silhouette score: {silhouette_score(data_scaled.reshape(n_series, -1), dtw_labels):.3f}\")\n",
    "\n",
    "# METHOD 2: Flatten and use traditional clustering\n",
    "print(\"\\n=== METHOD 2: Flattened Features + K-Means ===\")\n",
    "\n",
    "# Flatten multivariate time series\n",
    "data_flat = data.reshape(n_series, -1)\n",
    "scaler_flat = StandardScaler()\n",
    "data_flat_scaled = scaler_flat.fit_transform(data_flat)\n",
    "\n",
    "# K-means on flattened data\n",
    "kmeans_flat = KMeans(n_clusters=3, random_state=42)\n",
    "flat_labels = kmeans_flat.fit_predict(data_flat_scaled)\n",
    "\n",
    "print(f\"Flattened K-means silhouette score: {silhouette_score(data_flat_scaled, flat_labels):.3f}\")\n",
    "\n",
    "# METHOD 3: Feature extraction + clustering\n",
    "print(\"\\n=== METHOD 3: Feature Engineering + Clustering ===\")\n",
    "\n",
    "def extract_features(ts):\n",
    "    \"\"\"Extract statistical features from multivariate time series\"\"\"\n",
    "    features = []\n",
    "    for var in range(ts.shape[1]):\n",
    "        series = ts[:, var]\n",
    "        features.extend([\n",
    "            np.mean(series),\n",
    "            np.std(series),\n",
    "            np.min(series),\n",
    "            np.max(series),\n",
    "            np.median(series),\n",
    "            np.percentile(series, 25),\n",
    "            np.percentile(series, 75),\n",
    "            # Trend (slope of linear regression)\n",
    "            np.polyfit(range(len(series)), series, 1)[0]\n",
    "        ])\n",
    "    return features\n",
    "\n",
    "# Extract features for all time series\n",
    "features = np.array([extract_features(ts) for ts in data])\n",
    "features_scaled = StandardScaler().fit_transform(features)\n",
    "\n",
    "# Clustering on features\n",
    "kmeans_features = KMeans(n_clusters=3, random_state=42)\n",
    "feature_labels = kmeans_features.fit_predict(features_scaled)\n",
    "\n",
    "print(f\"Feature-based K-means silhouette score: {silhouette_score(features_scaled, feature_labels):.3f}\")\n",
    "\n",
    "# METHOD 4: PCA + Clustering\n",
    "print(\"\\n=== METHOD 4: PCA Dimensionality Reduction + Clustering ===\")\n",
    "\n",
    "# Apply PCA to flattened data\n",
    "pca = PCA(n_components=0.95)  # Keep 95% of variance\n",
    "data_pca = pca.fit_transform(data_flat_scaled)\n",
    "\n",
    "# K-means on PCA components\n",
    "kmeans_pca = KMeans(n_clusters=3, random_state=42)\n",
    "pca_labels = kmeans_pca.fit_predict(data_pca)\n",
    "\n",
    "print(f\"PCA + K-means silhouette score: {silhouette_score(data_pca, pca_labels):.3f}\")\n",
    "print(f\"PCA components retained: {data_pca.shape[1]} (explaining {pca.explained_variance_ratio_.sum():.3f} of variance)\")\n",
    "\n",
    "# VISUALIZATION\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "methods = [\n",
    "    (\"True Labels\", np.concatenate([np.zeros(33), np.ones(33), np.full(34, 2)])),\n",
    "    (\"DTW K-means\", dtw_labels),\n",
    "    (\"Flattened K-means\", flat_labels), \n",
    "    (\"Feature K-means\", feature_labels),\n",
    "    (\"PCA K-means\", pca_labels)\n",
    "]\n",
    "\n",
    "# Plot first variable for each method\n",
    "for idx, (method_name, labels) in enumerate(methods):\n",
    "    if idx < 4:\n",
    "        ax = axes[0, idx]\n",
    "    else:\n",
    "        ax = axes[1, 0]\n",
    "    \n",
    "    colors = ['red', 'blue', 'green', 'purple', 'orange']\n",
    "    for i, ts in enumerate(data[:30]):  # Plot first 30 series\n",
    "        ax.plot(ts[:, 0], color=colors[int(labels[i])], alpha=0.3)\n",
    "    ax.set_title(f'{method_name}\\n(Variable 1)')\n",
    "    ax.set_xlabel('Time')\n",
    "\n",
    "# Plot cluster comparison\n",
    "axes[1, 1].axis('off')\n",
    "comparison_df = pd.DataFrame({\n",
    "    'True': np.concatenate([np.zeros(33), np.ones(33), np.full(34, 2)]),\n",
    "    'DTW': dtw_labels,\n",
    "    'Flattened': flat_labels,\n",
    "    'Features': feature_labels,\n",
    "    'PCA': pca_labels\n",
    "})\n",
    "\n",
    "# Confusion matrix between true and best method\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "scores = {}\n",
    "for method in ['DTW', 'Flattened', 'Features', 'PCA']:\n",
    "    scores[method] = adjusted_rand_score(comparison_df['True'], comparison_df[method])\n",
    "\n",
    "print(f\"\\nAdjusted Rand Index (vs True Labels):\")\n",
    "for method, score in scores.items():\n",
    "    print(f\"{method}: {score:.3f}\")\n",
    "\n",
    "# Plot silhouette comparison\n",
    "methods_scores = {\n",
    "    'DTW': silhouette_score(data_scaled.reshape(n_series, -1), dtw_labels),\n",
    "    'Flattened': silhouette_score(data_flat_scaled, flat_labels),\n",
    "    'Features': silhouette_score(features_scaled, feature_labels),\n",
    "    'PCA': silhouette_score(data_pca, pca_labels)\n",
    "}\n",
    "\n",
    "axes[1, 2].bar(methods_scores.keys(), methods_scores.values())\n",
    "axes[1, 2].set_title('Silhouette Scores')\n",
    "axes[1, 2].set_ylabel('Score')\n",
    "\n",
    "# Feature importance plot (for feature-based method)\n",
    "if len(features) > 0:\n",
    "    feature_names = []\n",
    "    for var in range(n_variables):\n",
    "        feature_names.extend([f'V{var}_mean', f'V{var}_std', f'V{var}_min', f'V{var}_max', \n",
    "                            f'V{var}_median', f'V{var}_q25', f'V{var}_q75', f'V{var}_trend'])\n",
    "    \n",
    "    # PCA components of features to see importance\n",
    "    pca_features = PCA(n_components=3)\n",
    "    pca_features.fit(features_scaled)\n",
    "    \n",
    "    axes[1, 3].bar(range(len(pca_features.explained_variance_ratio_)), \n",
    "                   pca_features.explained_variance_ratio_)\n",
    "    axes[1, 3].set_title('Feature PCA Explained Variance')\n",
    "    axes[1, 3].set_xlabel('Component')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# RECOMMENDATIONS\n",
    "print(\"\\n=== RECOMMENDATIONS ===\")\n",
    "print(\"1. DTW K-means: Best for preserving temporal patterns and shape similarity\")\n",
    "print(\"2. Feature-based: Good interpretability, fast, works well with domain knowledge\")\n",
    "print(\"3. Flattened + PCA: Good compromise between speed and performance\")\n",
    "print(\"4. For large datasets: Use feature extraction or PCA first\")\n",
    "print(\"5. Consider hierarchical clustering for better cluster interpretability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example usage\n",
    "# Create a test directed graph\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from([\n",
    "    (1, 2, {'weight': 3}),\n",
    "    (2, 3, {'weight': 5}),\n",
    "    (3, 4, {'weight': 2}),\n",
    "    (4, 1, {'weight': 4}),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from huggingface_hub import notebook_login\n",
    "#notebook_login()\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import logging\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "# Example text\n",
    "text1 = \"Renewable energy is the future.\"\n",
    "text2 = \"However, it cannot fully replace fossil fuels due to reliability issues.\"\n",
    "text1 = \"Earth is not flat\"\n",
    "text2 = \"Der Hahn kr√§ht\"\n",
    "text2 = \"Atomar power is not safe\"\n",
    "\n",
    "# Detect fake news\n",
    "model_name = \"openai-community/roberta-base-openai-detector\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "result = pipe(text1)\n",
    "print(result)\n",
    "\n",
    "# Relation classification model\n",
    "relation_pipeline = pipeline(\"text-classification\", model=\"raruidol/ArgumentMining-EN-ARI-AIF-RoBERTa_L\")\n",
    "result = relation_pipeline(f\"{text1} [SEP] {text2}\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!pip install textdescriptives\n",
    "#!spacy download en_core_web_sm\n",
    "import spacy\n",
    "import textdescriptives as td\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"textdescriptives/quality\")\n",
    "doc = nlp(\"The world is changed. I feel it in the \\n water. I feel it in\\n the earth. I smell it in the air. Much that once was is lost, for none now live who remember it.\")\n",
    "\n",
    "# all attributes are stored as a dict in the ._.quality attribute\n",
    "doc._.quality\n",
    "# check if the document passed all quality checks\n",
    "doc._.passed_quality_check\n",
    "\n",
    "# extract to dataframe\n",
    "td.extract_df(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import json\n",
    "expected_fields=[\"study_subject\", \"status\", \"comment\"]\n",
    "llm_message = '{\"status\": \"welt\", \"comment\":\"nonnne\", \"study_subject\": \"math\"}'\n",
    "regex = r\"{[\\s\\S]+}\"\n",
    "print(re.search(regex, llm_message))\n",
    "json_string = re.search(regex, llm_message).group()\n",
    "json_output = json.loads(json_string)\n",
    "for field in expected_fields:\n",
    "    if not json_output[field]:\n",
    "        continue\n",
    "json_valid = True\n",
    "print(json_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
