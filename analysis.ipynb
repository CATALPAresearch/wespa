{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from src.util import prnt\n",
    "from src.util import print_all_output\n",
    "from src.settings import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.load import Load\n",
    "semester = 'SS2024'\n",
    "# Step 1\n",
    "l = Load(semester)\n",
    "l.process_textedits(filter_weeks=1)\n",
    "df = l.get_data()\n",
    "#l.run(filter_weeks=1)\n",
    "df\n",
    "# load all data of a group\n",
    "#l.run(filter_group=[2751],filter_weeks=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: EasySync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "\n",
    "from src.extract_easy_sync import Extract_Easy_Sync\n",
    "semester = 'SS2024'\n",
    "# Step 2\n",
    "#print_all_output = False\n",
    "es = Extract_Easy_Sync(semester)\n",
    "es.is_reconstructing_text = False\n",
    "df_textchanges = es.extract_easy_sync(df)\n",
    "df_textchanges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2.1: Extract sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.extract_sessions import Extract_Sessions\n",
    "\n",
    "semester = 'SS2024'\n",
    "es = Extract_Sessions(semester)\n",
    "#dd = es.load_and_combine_df() # TODO: All authorids in the pad_commit file are derived from moodle but not from etherpad\n",
    "#dd['authorid'].unique()\n",
    "#dd[dd['authorid'].isnull()]\n",
    "df_textchanges.columns\n",
    "df_sessions = es.extract_sessions(df_textchanges)\n",
    "df_sessions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2.2: Extract and Analyze Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# TESTING ONLY\n",
    "\n",
    "from src.text_quality import Preprocess_Text_Quality\n",
    "from src.extract_easy_sync import Extract_Easy_Sync\n",
    "\n",
    "semester = 'SS2024'\n",
    "\n",
    "# Step 1: identify time breaks\n",
    "ptq = Preprocess_Text_Quality(semester)\n",
    "sft = semester_form_to_arr[semester]  # variable in settings.py\n",
    "time_breaks = ptq.split_text_progression_by_threshold(\n",
    "    #df[df['moodle_group_id']==3925], \n",
    "    df,\n",
    "    semester, \n",
    "    start_date=datetime(sft[0][0], sft[0][1], sft[0][2]), \n",
    "    end_date=datetime(sft[1][0], sft[1][1], sft[1][2]),\n",
    "    group_column='moodle_pad_id',\n",
    "    threshold_type='days'\n",
    "    )\n",
    "time_breaks.sort_values(by=['moodle_group_id', 'moodle_pad_id'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.text_quality import Preprocess_Text_Quality\n",
    "from src.extract_easy_sync import Extract_Easy_Sync\n",
    "\n",
    "semester = 'SS2024'\n",
    "timestamp_threshold_type='days'\n",
    "\n",
    "# Step 1: identify time breaks\n",
    "ptq = Preprocess_Text_Quality(semester)\n",
    "sft = semester_form_to_arr[semester]  # variable in settings.py\n",
    "time_breaks = ptq.split_text_progression_by_threshold(\n",
    "    #df[df['moodle_group_id']==3925], \n",
    "    df,\n",
    "    semester, \n",
    "    start_date=datetime(sft[0][0], sft[0][1], sft[0][2]), \n",
    "    end_date=datetime(sft[1][0], sft[1][1], sft[1][2]),\n",
    "    group_column='moodle_pad_id',\n",
    "    threshold_type=timestamp_threshold_type\n",
    "    )\n",
    "time_breaks.sort_values(by=['moodle_group_id', 'moodle_pad_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.text_quality import Preprocess_Text_Quality, Text_Quality\n",
    "from src.extract_easy_sync import Extract_Easy_Sync\n",
    "import de_core_news_sm\n",
    "\n",
    "text = \"Der Mann geht Joggen.\"\n",
    "tq = Text_Quality()\n",
    "tq.run(text, model='de_core_news_lg') # 'de_core_news_md'\n",
    "\n",
    "import de_core_news_sm\n",
    "\n",
    "nlp = de_core_news_sm.load()\n",
    "\n",
    "tokenized = nlp(text)\n",
    "for token in tokenized:\n",
    "    print(token, token.pos_, token.ent_type_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# TESTINg ONLY\n",
    "\n",
    "timestamp_threshold_type='days'\n",
    "\n",
    "es = Extract_Easy_Sync(semester, time_breaks=time_breaks)\n",
    "es.is_reconstructing_text = True\n",
    "print_all_output = True\n",
    "\n",
    "allsets = [\n",
    "    \"Z:0>12=0+12$Hello World\",\n",
    "    \"Z:12>3=6+3$you \",  \n",
    "    \"Z:15>1=10-1+1$!\",\n",
    "    \"Z:0=15|10+1$\"\n",
    "]\n",
    "\n",
    "pads = df[df['moodle_pad_id']==df['moodle_pad_id'].unique()[14]].groupby('moodle_pad_id', group_keys=True)\n",
    "#pads = df[df['moodle_pad_id']==df['moodle_pad_id'].unique()].groupby('moodle_pad_id', group_keys=True)\n",
    "\n",
    "i = 0\n",
    "for index, pad in pads:\n",
    "    print(' ')\n",
    "    for row in pad.sort_values(by=['timestamp']).itertuples():\n",
    "        i = i + 1\n",
    "        if i > 0:\n",
    "            es.extract_changeset(\n",
    "                row[5],             # row['textedit_changeset'], \n",
    "                timestamp=row[6],   # row['timestamp'], \n",
    "                group_id=row[3],    # row['moodle_group_id'], \n",
    "                pad_id=row[4],      # str(row['moodle_pad_id'])\n",
    "            )\n",
    "    print(es.ttext)\n",
    "\n",
    "\n",
    "ptq = Preprocess_Text_Quality(semester)\n",
    "res_tq = ptq.determine_text_quality_from_files(threshold_type=timestamp_threshold_type)\n",
    "res_tq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.text_quality import Preprocess_Text_Quality\n",
    "from src.extract_easy_sync import Extract_Easy_Sync\n",
    "\n",
    "semester = 'SS2024'\n",
    "\n",
    "# Step 1: identify time breaks\n",
    "ptq = Preprocess_Text_Quality(semester)\n",
    "sft = semester_form_to_arr[semester]  # variable in settings.py\n",
    "time_breaks = ptq.split_text_progression_by_threshold(\n",
    "    #df[df['moodle_group_id']==3925], \n",
    "    df,\n",
    "    semester, \n",
    "    start_date=datetime(sft[0][0], sft[0][1], sft[0][2]), \n",
    "    end_date=datetime(sft[1][0], sft[1][1], sft[1][2]),\n",
    "    group_column='moodle_pad_id',\n",
    "    threshold_type='days'\n",
    "    )\n",
    "time_breaks.sort_values(by=['moodle_group_id', 'moodle_pad_id'])\n",
    "\n",
    "# Step 2: Extract text at the time breaks\n",
    "#df = df.dropna(subset=[\"moodle_pad_id\"])\n",
    "es = Extract_Easy_Sync(semester, time_breaks=time_breaks)\n",
    "es.is_reconstructing_text = True\n",
    "\n",
    "pads = df[df['moodle_pad_id']==df['moodle_pad_id'].unique()].groupby('moodle_pad_id', group_keys=True)\n",
    "\n",
    "i = 0\n",
    "for index, pad in pads:\n",
    "    print(' ')\n",
    "    for row in pad.sort_values(by=['timestamp']).itertuples():\n",
    "        i = i + 1\n",
    "        if i > 0:\n",
    "            es.extract_changeset(\n",
    "                row[5],             # row['textedit_changeset'], \n",
    "                timestamp=row[6],   # row['timestamp'], \n",
    "                group_id=row[3],    # row['moodle_group_id'], \n",
    "                pad_id=row[4],      # str(row['moodle_pad_id'])\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Extract text at the time breaks\n",
    "#df = df.dropna(subset=[\"moodle_pad_id\"])\n",
    "es = Extract_Easy_Sync(semester, time_breaks=time_breaks)\n",
    "es.is_reconstructing_text = True\n",
    "#pads = df[df['moodle_group_id']==df['moodle_group_id'].unique()[11]].groupby('moodle_pad_id', group_keys=True)\n",
    "pads = df[df['moodle_pad_id']==df['moodle_pad_id'].unique()[11]].groupby('moodle_pad_id', group_keys=True)\n",
    "for index, pad in pads:\n",
    "    #print(group['moodle_pad_id'])\n",
    "    for row in pad.sort_values(by=['timestamp']).itertuples():\n",
    "        #print(row[1], row[2],row[3],row[4],row[5],row[6],)\n",
    "        es.extract_changeset(\n",
    "            row[5], #row['textedit_changeset'], \n",
    "            timestamp=row[6], #row['timestamp'], \n",
    "            group_id=row[3], # row['moodle_group_id'], \n",
    "            pad_id=row[4], # str(row['moodle_pad_id'])\n",
    "        )\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_textchanges = (\n",
    "    df[df['moodle_group_id']==df['moodle_group_id'].unique()[10]]\n",
    "    .groupby('moodle_pad_id', group_keys=True)  # Group by pad ID\n",
    "    .apply(\n",
    "        lambda group: group.assign(\n",
    "            week=group['timestamp'].apply(\n",
    "                lambda ts: pd.to_datetime(ts, unit='s').strftime(\"%y-%U\")\n",
    "            ),\n",
    "            **group.apply(lambda row: \n",
    "                es.extract_changeset(\n",
    "                    row['textedit_changeset'], \n",
    "                    timestamp=row['timestamp'], \n",
    "                    group_id=row['moodle_group_id'], \n",
    "                    pad_id=str(row['moodle_pad_id'])\n",
    "                ), axis=1).apply(pd.Series),\n",
    "        )\n",
    "    )\n",
    ")\n",
    "es.save_data(df_textchanges, 'textchanges.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.text_quality import Preprocess_Text_Quality\n",
    "from src.extract_easy_sync import Extract_Easy_Sync\n",
    "\n",
    "semester = 'SS2024'\n",
    "\n",
    "# Step 1: identify time breaks\n",
    "ptq = Preprocess_Text_Quality(semester)\n",
    "sft = semester_form_to_arr[semester]  # variable in settings.py\n",
    "time_breaks = ptq.split_text_progression_by_threshold(\n",
    "    df, \n",
    "    semester, \n",
    "    datetime(sft[0][0], sft[0][1], sft[0][2]), \n",
    "    datetime(sft[1][0], sft[1][1], sft[1][2]),\n",
    "    group_column='moodle_group_id',\n",
    "    threshold_type='weeks'\n",
    "    )\n",
    "time_breaks = time_breaks[['id', 'moodle_group_id', 'moodle_author_id', 'moodle_pad_id', 'timestamp']]\n",
    "\n",
    "# Step 2: Extract text at the time breaks\n",
    "#df = df.dropna(subset=[\"moodle_pad_id\"])\n",
    "es = Extract_Easy_Sync(semester, time_breaks=time_breaks)\n",
    "es.is_reconstructing_text = True\n",
    "df_textchanges = es.extract_easy_sync(df)\n",
    "df_textchanges\n",
    "\n",
    "# Step 3: Compute text features\n",
    "df_raw_text = pd.read_csv(f'{output_path}{project_name}-{semester}-02.1-text-revisions.csv')\n",
    "df_raw_text.columns = [\n",
    "    \"group_id\", \n",
    "    \"pad_id\",\n",
    "    \"timestamp\", \n",
    "    \"current_time_threshold\",\n",
    "    \"text\",\n",
    "]\n",
    "res = ptq.determine_text_quality(df_raw_text) # will be saved to file 2.2\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "#df_raw_text = pd.read_csv(f'{output_path}{project_name}-{semester}-02.1-text-revisions.csv')\n",
    "df_raw_text = pd.read_csv(f'{output_path}{project_name}-{semester}-02.1-text-quality-new.csv')\n",
    "df_raw_text.columns = [\n",
    "    \"group_id\", \n",
    "    \"pad_id\",\n",
    "    \"timestamp\", \n",
    "    \"current_time_threshold\",\n",
    "    \"text\",\n",
    "]\n",
    "print(\n",
    "    len(df_raw_text['group_id'].unique()),\n",
    "    len(df_raw_text['timestamp'].unique()),\n",
    "    df_raw_text.text.str.len().mean(),\n",
    "    df_raw_text.shape[0]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 : Extract_Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from src.extract_neighbours import Extract_Neighbours\n",
    "\n",
    "# Step 3\n",
    "print_all_output=False\n",
    "semester = 'SS2024'\n",
    "en = Extract_Neighbours(semester)\n",
    "author_relations = en.extract_neighbours(df_textchanges)\n",
    "author_relations\n",
    "\n",
    "#author_relations[author_relations['right_neighbor']>0]\n",
    "#df_textchanges['moodle_pad_id_'] = df_textchanges['moodle_pad_id'].str.split('$', expand=True)[0]\n",
    "#pads = df_textchanges['moodle_pad_id_'].unique()\n",
    "#df_textchanges[df_textchanges['moodle_pad_id_'] == pads[3]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Extract_Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.extract_degrees import Extract_Degree\n",
    "\n",
    "# Step 4\n",
    "semester = 'SS2024'\n",
    "ed = Extract_Degree(semester)\n",
    "author_relations_summary = ed.summarize_individual_level(author_relations)\n",
    "\n",
    "#author_relations_summary\n",
    "#author_relations_summary['right2'] = author_relations_summary['right'].astype('int').abs() \n",
    "#author_relations_summary['right2']\n",
    "#author_relations_summary.dtypes\n",
    "\n",
    "print_all_output = False #True\n",
    "author_degrees = ed.extract_degree(author_relations_summary)\n",
    "# TODO: During the processing the size of the dataframe varys pretty much. Something is wrong with the agg function. Check if this behavior is ok.\n",
    "#print(author_relations_summary.size)\n",
    "#print(author_degrees.size)\n",
    "#author_degrees\n",
    "\n",
    "print_all_output = True\n",
    "author_degrees_per_group = ed.map_to_group(df_textchanges, author_degrees)\n",
    "#print('df_textchanges', df_textchanges)\n",
    "#print('author_degrees', author_degrees)\n",
    "#author_degrees_per_group[author_degrees_per_group['indegree_count']>0]\n",
    "author_degrees_per_group\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Coll Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.collaboration_graph import Collaboration_Graph\n",
    "\n",
    "semester = 'SS2024'\n",
    "cg = Collaboration_Graph(semester)\n",
    "#cg.check_random_group(author_relations)\n",
    "graph_measures = cg.create_graph_for_all_groups(author_relations, save_plot=False, save_output=True, show_plot=False)\n",
    "graph_measures\n",
    "#cg.create_json_graph_for_all_groups(author_relations, last_modified = 1733786132)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Run over all semesters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process semester SS2024\n",
      "['24-16', '24-17'] ['24-18', '24-19', '24-20'] ['24-21', '24-22', '24-23']\n",
      "Loading textedit data from CSV\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\"\"\"Step 0: Run over all semesters\"\"\"\n",
    "\n",
    "from src.settings import *\n",
    "from src.load import Load\n",
    "from src.extract_easy_sync import Extract_Easy_Sync\n",
    "from src.extract_sessions import Extract_Sessions\n",
    "from src.extract_neighbours import Extract_Neighbours\n",
    "from src.extract_degrees import Extract_Degree\n",
    "from src.collaboration_graph import Collaboration_Graph\n",
    "from src.text_quality import Preprocess_Text_Quality\n",
    "\n",
    "semesters = [\n",
    "    # not available # 'SS2021', \n",
    "    #('WS2021_22', 'dump20240826'),\n",
    "    #('SS2022', 'dump20240826'),\n",
    "    #('WS2022_23', 'dump20240826'),\n",
    "    #('SS2023', 'dump20240826'),\n",
    "    #('WS2023_24', 'dump20240826'),\n",
    "    ('SS2024', 'dump20240826'),\n",
    "    #('WS2024_25', 'dump20241209'),\n",
    "]\n",
    "print_all_output = False\n",
    "compute_texts = False\n",
    "for the_semester in semesters:\n",
    "    print('Process semester '+ the_semester[0])\n",
    "    semester = the_semester[0]\n",
    "    the_dump = the_semester[1]\n",
    "    period_1 = get_period(semester, period_1_arr)\n",
    "    period_2 = get_period(semester, period_2_arr)\n",
    "    period_3 = get_period(semester, period_3_arr)\n",
    "    print(period_1, period_2, period_3)\n",
    "    # Step 1\n",
    "    l = Load(semester, the_dump)\n",
    "    #l.run()\n",
    "    l.process_textedits(filter_weeks=1)\n",
    "    df = l.get_data()\n",
    "    # Step 2\n",
    "    # Step 2.1: identify time breaks\n",
    "    ptq = Preprocess_Text_Quality(semester)\n",
    "    sft = semester_form_to_arr[semester]  # variable in settings.py\n",
    "    time_breaks = ptq.split_text_progression_by_threshold(\n",
    "        #df[df['moodle_group_id']==3925], \n",
    "        df,\n",
    "        semester, \n",
    "        start_date=datetime(sft[0][0], sft[0][1], sft[0][2]), \n",
    "        end_date=datetime(sft[1][0], sft[1][1], sft[1][2]),\n",
    "        group_column='moodle_pad_id',\n",
    "        threshold_type='days'\n",
    "        )\n",
    "    time_breaks.sort_values(by=['moodle_group_id', 'moodle_pad_id'])\n",
    "    # Step 2.2 Extract changesets\n",
    "    es = Extract_Easy_Sync(semester, time_breaks)\n",
    "    es.is_reconstructing_text = compute_texts\n",
    "    df_textchanges = es.extract_easy_sync(df)\n",
    "    # Step 3\n",
    "    ##es = Extract_Sessions(semester)\n",
    "    ##df_sessions = es.extract_sessions(df_textchanges)\n",
    "    # Step 4\n",
    "    if compute_texts == True:\n",
    "        all_text = pd.read_csv(f'{output_path}{project_name}-{semester}-02.1-text-revisions.csv')\n",
    "        ptq = Preprocess_Text_Quality(semester)\n",
    "        sft = semester_form_to_arr[semester]  # variable in settings.py\n",
    "        df_raw_text = ptq.split_text_progression_by_threshold(\n",
    "            all_text, \n",
    "            semester, \n",
    "            datetime(sft[0][0], sft[0][1], sft[0][2]), \n",
    "            datetime(sft[1][0], sft[1][1], sft[1][2])\n",
    "            )\n",
    "        res = ptq.determine_text_quality(df_raw_text) # will be saved to file 2.2\n",
    "    \n",
    "    en = Extract_Neighbours(semester)\n",
    "    ed = Extract_Degree(semester)\n",
    "    df_textchanges['timestamp'] = df_textchanges['timestamp'].astype(float)\n",
    "    date_thresholds = es.generate_observation_times(\n",
    "        datetime(sft[0][0], sft[0][1], sft[0][2]), \n",
    "        datetime(sft[1][0], sft[1][1], sft[1][2]),\n",
    "        threshold_type='days'\n",
    "        )\n",
    "    for subset_until in date_thresholds:\n",
    "        #print(subset_until)\n",
    "        df_textchanges_until_break = df_textchanges[df_textchanges['timestamp'] < subset_until]\n",
    "        if df_textchanges_until_break.empty():\n",
    "            continue\n",
    "        # Step 5\n",
    "        author_relations = en.extract_neighbours(df_textchanges_until_break, subset_until) # before: df_textchanges xxx\n",
    "        # Step 6\n",
    "        author_relations_summary = ed.summarize_individual_level(author_relations, subset_until)\n",
    "        author_degrees = ed.extract_degree(author_relations_summary, subset_until)\n",
    "        #print(df_textchanges_until_break)\n",
    "        #print(author_degrees)\n",
    "        author_degrees_per_group = ed.map_to_group(df_textchanges_until_break, author_degrees, subset_until)  \n",
    "        # Step 7\n",
    "        cg = Collaboration_Graph(semester)\n",
    "        graph_measures = cg.create_graph_for_all_groups(author_relations, subset_until, save_plot=True, save_output=True, show_plot=False)\n",
    "        # fin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10134\n",
      "14333\n",
      "17869\n",
      "18551\n",
      "19313\n",
      "22599\n",
      "31900\n",
      "37623\n",
      "39773\n",
      "48945\n",
      "59271\n",
      "68421\n",
      "69375\n",
      "82661\n",
      "97529\n",
      "110705\n",
      "121359\n",
      "134024\n",
      "150506\n",
      "165336\n",
      "196480\n",
      "201184\n",
      "207173\n",
      "209175\n",
      "211917\n",
      "219162\n",
      "222590\n",
      "228076\n",
      "234161\n",
      "239136\n",
      "245019\n",
      "251035\n",
      "257605\n",
      "260491\n",
      "265994\n",
      "268838\n",
      "274084\n",
      "283268\n",
      "286719\n",
      "295403\n",
      "302073\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n",
      "302163\n"
     ]
    }
   ],
   "source": [
    "#df_textchanges_until_break\n",
    "#author_degrees\n",
    "#author_relations\n",
    "date_thresholds = np.array(date_thresholds)\n",
    "for i in date_thresholds:\n",
    "        df_textchanges_until_break = df_textchanges[df_textchanges['timestamp'] < i]\n",
    "        if df_textchanges_until_break.empty:\n",
    "            #print(i)\n",
    "            continue\n",
    "        print(df_textchanges_until_break.shape[0])\n",
    "\n",
    "#df_textchanges[df_textchanges['timestamp'] < max(date_thresholds)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example usage\n",
    "# Create a test directed graph\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from([\n",
    "    (1, 2, {'weight': 3}),\n",
    "    (2, 3, {'weight': 5}),\n",
    "    (3, 4, {'weight': 2}),\n",
    "    (4, 1, {'weight': 4}),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 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\n",
    "from huggingface_hub import notebook_login\n",
    "#notebook_login()\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import logging\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "# Example text\n",
    "text1 = \"Renewable energy is the future.\"\n",
    "text2 = \"However, it cannot fully replace fossil fuels due to reliability issues.\"\n",
    "text1 = \"Earth is not flat\"\n",
    "text2 = \"Der Hahn kräht\"\n",
    "text2 = \"Atomar power is not safe\"\n",
    "\n",
    "# Detect fake news\n",
    "model_name = \"openai-community/roberta-base-openai-detector\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "result = pipe(text1)\n",
    "print(result)\n",
    "\n",
    "# Relation classification model\n",
    "relation_pipeline = pipeline(\"text-classification\", model=\"raruidol/ArgumentMining-EN-ARI-AIF-RoBERTa_L\")\n",
    "result = relation_pipeline(f\"{text1} [SEP] {text2}\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!pip install textdescriptives\n",
    "#!spacy download en_core_web_sm\n",
    "import spacy\n",
    "import textdescriptives as td\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"textdescriptives/quality\")\n",
    "doc = nlp(\"The world is changed. I feel it in the \\n water. I feel it in\\n the earth. I smell it in the air. Much that once was is lost, for none now live who remember it.\")\n",
    "\n",
    "# all attributes are stored as a dict in the ._.quality attribute\n",
    "doc._.quality\n",
    "# check if the document passed all quality checks\n",
    "doc._.passed_quality_check\n",
    "\n",
    "# extract to dataframe\n",
    "td.extract_df(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
