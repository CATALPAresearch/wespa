{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from src.util import prnt\n",
    "from src.util import print_all_output\n",
    "from src.settings import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.load import Load\n",
    "semester = 'WS2023_24' #('WS2023_24',   'dump20240826'),\n",
    "# Step 1\n",
    "l = Load(semester)\n",
    "l.process_textedits(filter_weeks=1)\n",
    "df = l.get_data()\n",
    "#l.run(filter_weeks=1)\n",
    "df\n",
    "# load all data of a group\n",
    "#l.run(filter_group=[2751],filter_weeks=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: EasySync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "\n",
    "from src.extract_easy_sync import Extract_Easy_Sync\n",
    "semester = 'WS2023_24'\n",
    "# Step 2\n",
    "#print_all_output = False\n",
    "sft = semester_form_to_arr[semester]  # variable in settings.py\n",
    "time_breaks = ptq.split_text_progression_by_threshold(\n",
    "    df,\n",
    "    semester, \n",
    "    start_date=datetime(sft[0][0], sft[0][1], sft[0][2]), \n",
    "    end_date=datetime(sft[1][0], sft[1][1], sft[1][2]),\n",
    "    group_column='moodle_pad_id',\n",
    "    period_split_interval=period_split_interval\n",
    "    )\n",
    "time_breaks.sort_values(by=['moodle_group_id', 'moodle_pad_id'])\n",
    "\n",
    "es = Extract_Easy_Sync(semester, time_breaks)\n",
    "es.is_reconstructing_text = False\n",
    "df_textchanges = es.extract_easy_sync(df)\n",
    "df_textchanges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2.1: Extract sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.extract_sessions import Extract_Sessions\n",
    "\n",
    "semester = 'SS2024'\n",
    "es = Extract_Sessions(semester)\n",
    "#dd = es.load_and_combine_df() # TODO: All authorids in the pad_commit file are derived from moodle but not from etherpad\n",
    "#dd['authorid'].unique()\n",
    "#dd[dd['authorid'].isnull()]\n",
    "df_textchanges.columns\n",
    "df_sessions = es.extract_sessions(df_textchanges)\n",
    "df_sessions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2.2: Extract and Analyze Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\"\"\"\n",
    "This script extracts all texts per group/pad and defined periods.\n",
    "The resulting text versions are stored as files. Then the text is analyzed for certain features.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from src.util import prnt\n",
    "from src.util import print_all_output\n",
    "from src.settings import *\n",
    "from src.text_quality import Preprocess_Text_Quality\n",
    "from src.extract_easy_sync import Extract_Easy_Sync\n",
    "\n",
    "semester = 'SS2024'\n",
    "period_split_interval='weeks'\n",
    "\n",
    "# Step 1: identify time breaks\n",
    "ptq = Preprocess_Text_Quality(semester)\n",
    "sft = semester_form_to_arr[semester]  # variable in settings.py\n",
    "time_breaks = ptq.split_text_progression_by_threshold(\n",
    "    #df[df['moodle_group_id']==3925], \n",
    "    df,\n",
    "    semester, \n",
    "    start_date=datetime(sft[0][0], sft[0][1], sft[0][2]), \n",
    "    end_date=datetime(sft[1][0], sft[1][1], sft[1][2]),\n",
    "    group_column='moodle_pad_id',\n",
    "    period_split_interval=period_split_interval\n",
    "    )\n",
    "time_breaks.sort_values(by=['moodle_group_id', 'moodle_pad_id'])\n",
    "\n",
    "\n",
    "es = Extract_Easy_Sync(semester, time_breaks=time_breaks, period_split_interval=period_split_interval)\n",
    "es.is_reconstructing_text = True\n",
    "print_all_output = True\n",
    "\n",
    "##pads = df[df['moodle_pad_id']==df['moodle_pad_id'].unique()[14]].groupby('moodle_pad_id', group_keys=True)\n",
    "pads = df.groupby('moodle_pad_id', group_keys=True)\n",
    "\n",
    "i = 0\n",
    "for index, pad in pads:\n",
    "    print(' ')\n",
    "    for row in pad.sort_values(by=['timestamp']).itertuples():\n",
    "        i = i + 1\n",
    "        if i > 0:\n",
    "            es.extract_changeset(\n",
    "                row[5],             # row['textedit_changeset'], \n",
    "                timestamp=row[6],   # row['timestamp'], \n",
    "                group_id=row[3],    # row['moodle_group_id'], \n",
    "                pad_id=row[4],      # str(row['moodle_pad_id'])\n",
    "            )\n",
    "    #print(es.ttext)\n",
    "\n",
    "ptq = Preprocess_Text_Quality(semester)\n",
    "res_tq = ptq.determine_text_quality_from_files(period_split_interval=period_split_interval)\n",
    "res_tq\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nlp test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.text_quality import Preprocess_Text_Quality, Text_Quality\n",
    "from src.extract_easy_sync import Extract_Easy_Sync\n",
    "import de_core_news_sm\n",
    "\n",
    "text = \"Der Mann geht Joggen.\"\n",
    "tq = Text_Quality()\n",
    "tq.run(text, model='de_core_news_lg') # 'de_core_news_md'\n",
    "\n",
    "import de_core_news_sm\n",
    "\n",
    "nlp = de_core_news_sm.load()\n",
    "\n",
    "tokenized = nlp(text)\n",
    "for token in tokenized:\n",
    "    print(token, token.pos_, token.ent_type_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 : Extract_Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from src.extract_neighbours import Extract_Neighbours\n",
    "\n",
    "# Step 3\n",
    "print_all_output=False\n",
    "semester = 'WS2023_24'\n",
    "en = Extract_Neighbours(semester)\n",
    "author_relations = en.extract_neighbours(df_textchanges)\n",
    "author_relations\n",
    "\n",
    "#author_relations[author_relations['right_neighbor']>0]\n",
    "#df_textchanges['moodle_pad_id_'] = df_textchanges['moodle_pad_id'].str.split('$', expand=True)[0]\n",
    "#pads = df_textchanges['moodle_pad_id_'].unique()\n",
    "#df_textchanges[df_textchanges['moodle_pad_id_'] == pads[3]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Extract_Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.extract_degrees import Extract_Degree\n",
    "\n",
    "# Step 4\n",
    "semester = 'SS2024'\n",
    "ed = Extract_Degree(semester)\n",
    "author_relations_summary = ed.summarize_individual_level(author_relations)\n",
    "\n",
    "#author_relations_summary\n",
    "#author_relations_summary['right2'] = author_relations_summary['right'].astype('int').abs() \n",
    "#author_relations_summary['right2']\n",
    "#author_relations_summary.dtypes\n",
    "\n",
    "print_all_output = False #True\n",
    "author_degrees = ed.extract_degree(author_relations_summary)\n",
    "# TODO: During the processing the size of the dataframe varys pretty much. Something is wrong with the agg function. Check if this behavior is ok.\n",
    "#print(author_relations_summary.size)\n",
    "#print(author_degrees.size)\n",
    "#author_degrees\n",
    "\n",
    "print_all_output = True\n",
    "author_degrees_per_group = ed.map_to_group(df_textchanges, author_degrees)\n",
    "#print('df_textchanges', df_textchanges)\n",
    "#print('author_degrees', author_degrees)\n",
    "#author_degrees_per_group[author_degrees_per_group['indegree_count']>0]\n",
    "author_degrees_per_group\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Coll Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.collaboration_graph import Collaboration_Graph\n",
    "\n",
    "semester = 'WS2024_25'\n",
    "cg = Collaboration_Graph(semester)\n",
    "#cg.check_random_group(author_relations)\n",
    "graph_measures = cg.create_graph_for_all_groups(author_relations, save_plot=False, save_output=True, show_plot=False)\n",
    "graph_measures\n",
    "#cg.create_json_graph_for_all_groups(author_relations, last_modified = 1733786132)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All at once: Group behavior in all semesters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Process semester WS2023_24\n",
      "  Periods:  ['23-42', '23-43'] ['23-44', '23-45', '23-46'] ['23-47', '23-48', '23-49']\n",
      "## step1 - Load data\n",
      "Loading textedit data from CSV\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\"\"\"Step 0: Run over all semesters\"\"\"\n",
    "import gc  # Import garbage collector\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from src.util import prnt\n",
    "from src.util import print_all_output\n",
    "from src.settings import *\n",
    "from src.load import Load\n",
    "from src.extract_easy_sync import Extract_Easy_Sync\n",
    "from src.extract_sessions import Extract_Sessions\n",
    "from src.extract_neighbours import Extract_Neighbours\n",
    "from src.extract_degrees import Extract_Degree\n",
    "from src.collaboration_graph import Collaboration_Graph\n",
    "from src.text_quality import Preprocess_Text_Quality\n",
    "\n",
    "semesters = [\n",
    "    # not available # 'SS2021', \n",
    "    ##('WS2021_22', 'dump20240826'),\n",
    "    ##('SS2022',    'dump20240826'),\n",
    "    ##('WS2022_23', 'dump20240826'),\n",
    "    ##('SS2023',    'dump20240826'),\n",
    "    ('WS2023_24',   'dump20240826'),\n",
    "    ##('SS2024',    'dump20240826'),\n",
    "    ##('WS2024_25',  'dump20241209'),\n",
    "]\n",
    "period_split_interval = 'weeks'\n",
    "print_all_output = False\n",
    "compute_texts = False\n",
    "for the_semester in semesters:\n",
    "    print('# Process semester '+ the_semester[0])\n",
    "    semester = the_semester[0]\n",
    "    the_dump = the_semester[1]\n",
    "    period_1 = get_period(semester, period_1_arr)\n",
    "    period_2 = get_period(semester, period_2_arr)\n",
    "    period_3 = get_period(semester, period_3_arr)\n",
    "    print('  Periods: ', period_1, period_2, period_3)\n",
    "    print('## step1 - Load data')\n",
    "    l = Load(semester, the_dump)\n",
    "    #l.run()\n",
    "    l.process_textedits(filter_weeks=1)\n",
    "    df = l.get_data()\n",
    "    \n",
    "    # Step 2\n",
    "    # Step 2.1: identify time breaks\n",
    "    ptq = Preprocess_Text_Quality(semester)\n",
    "    sft = semester_form_to_arr[semester]  # variable in settings.py\n",
    "    time_breaks = ptq.split_text_progression_by_threshold(\n",
    "        df,\n",
    "        semester, \n",
    "        start_date=datetime(sft[0][0], sft[0][1], sft[0][2]), \n",
    "        end_date=datetime(sft[1][0], sft[1][1], sft[1][2]),\n",
    "        group_column='moodle_pad_id',\n",
    "        period_split_interval=period_split_interval\n",
    "        )\n",
    "    time_breaks.sort_values(by=['moodle_group_id', 'moodle_pad_id'])\n",
    "    \n",
    "    print('## step2 - Extract_Easy_Sync')\n",
    "    es = Extract_Easy_Sync(semester, time_breaks)\n",
    "    es.is_reconstructing_text = compute_texts\n",
    "    df_textchanges = es.extract_easy_sync(df)\n",
    "    del df  # Free memory\n",
    "    gc.collect()  # Force garbage collection\n",
    "    \n",
    "    print('## step3 Extract Sessions (skipped)') # TODO\n",
    "    ##es = Extract_Sessions(semester)\n",
    "    ##df_sessions = es.extract_sessions(df_textchanges)\n",
    "    \n",
    "    # Step 4\n",
    "    print('## step4 - Extract Text Versions (optional)')\n",
    "    if compute_texts == True:\n",
    "        all_text = pd.read_csv(f'{output_path}{project_name}-{semester}-02.1-text-revisions.csv')\n",
    "        ptq = Preprocess_Text_Quality(semester)\n",
    "        #sft = semester_form_to_arr[semester]  # variable in settings.py\n",
    "        df_raw_text = ptq.split_text_progression_by_threshold(\n",
    "            all_text, \n",
    "            semester, \n",
    "            datetime(sft[0][0], sft[0][1], sft[0][2]), \n",
    "            datetime(sft[1][0], sft[1][1], sft[1][2])\n",
    "            )\n",
    "        res = ptq.determine_text_quality(df_raw_text) # will be saved to file 2.2\n",
    "    \n",
    "    \n",
    "    en = Extract_Neighbours(semester, period_split_interval)\n",
    "    ed = Extract_Degree(semester, period_split_interval)\n",
    "    df_textchanges['timestamp'] = df_textchanges['timestamp'].astype(float)\n",
    "    date_thresholds = es.generate_observation_times(\n",
    "        datetime(sft[0][0], sft[0][1], sft[0][2]), \n",
    "        datetime(sft[1][0], sft[1][1], sft[1][2]),\n",
    "        period_split_interval=period_split_interval\n",
    "        )\n",
    "    \n",
    "    for subset_until in date_thresholds:\n",
    "        print(f'## process period {list(date_thresholds).index(subset_until)+1}/{len(date_thresholds)}')\n",
    "        df_textchanges_until_break = df_textchanges[df_textchanges['timestamp'] < subset_until]\n",
    "        if df_textchanges_until_break.empty:\n",
    "            continue\n",
    "        \n",
    "        print('### step5.1 - Extract_Neighbours')\n",
    "        author_relations = en.extract_neighbours(df_textchanges_until_break, subset_until) # before: df_textchanges xxx\n",
    "        \n",
    "        print('### step5.2 - Extract Degree')\n",
    "        author_relations_summary = ed.summarize_individual_level(author_relations, subset_until)\n",
    "        author_degrees = ed.extract_degree(author_relations_summary, subset_until)\n",
    "        author_degrees_per_group = ed.map_to_group(df_textchanges_until_break, author_degrees, subset_until)  \n",
    "        del df_textchanges_until_break, author_degrees_per_group, author_relations_summary\n",
    "        gc.collect()\n",
    "        \n",
    "        print('### step6 - Collaboration_Graph')\n",
    "        cg = Collaboration_Graph(semester, period_split_interval)\n",
    "        # TODO: save_plot imply displaying the plot\n",
    "        graph_measures = cg.create_graph_for_all_groups(author_relations, subset_until, save_plot=False, save_output=True, show_plot=False)\n",
    "        del author_relations, author_degrees, cg, graph_measures\n",
    "        gc.collect()\n",
    "    # fin\n",
    "\n",
    "    del df_textchanges\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All at once: Text features for all text versions in all semesters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\"\"\"\n",
    "This script iterates over all semesters and loads the textedit data from csv and extracts the EasySync data. \n",
    "During the extraction versions of the text documents are reconstructed for provided periods (e.g. per day or week). \n",
    "The resulting text version is stored as txt file in the output folder\n",
    "\"\"\"\n",
    "\n",
    "import gc  # Import garbage collector\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from src.util import prnt\n",
    "from src.util import print_all_output\n",
    "from src.settings import *\n",
    "from src.load import Load\n",
    "from src.extract_easy_sync import Extract_Easy_Sync\n",
    "from src.text_quality import Preprocess_Text_Quality\n",
    "\n",
    "semesters = [\n",
    "    # not available # 'SS2021', \n",
    "    ##('WS2021_22', 'dump20240826'),\n",
    "    ##('SS2022',    'dump20240826'),\n",
    "    ##('WS2022_23', 'dump20240826'),\n",
    "    ##('SS2023',    'dump20240826'),\n",
    "    ##('WS2023_24',   'dump20240826'),\n",
    "    ##('SS2024',    'dump20240826'),\n",
    "    ('WS2024_25',  'dump20241209'),\n",
    "]\n",
    "period_split_interval = 'weeks'\n",
    "print_all_output = False\n",
    "compute_texts = False\n",
    "for the_semester in semesters:\n",
    "    print('# Process semester '+ the_semester[0])\n",
    "    semester = the_semester[0]\n",
    "    the_dump = the_semester[1]\n",
    "    period_1 = get_period(semester, period_1_arr)\n",
    "    period_2 = get_period(semester, period_2_arr)\n",
    "    period_3 = get_period(semester, period_3_arr)\n",
    "    print('  Periods: ', period_1, period_2, period_3)\n",
    "    print('## step1 - Load data')\n",
    "    l = Load(semester, the_dump)\n",
    "    #l.run()\n",
    "    l.process_textedits(filter_weeks=1)\n",
    "    df = l.get_data()\n",
    "\n",
    "    print(f'## step2 - identify time breaks in semester {the_semester[0]}') \n",
    "    ptq = Preprocess_Text_Quality(semester)\n",
    "    sft = semester_form_to_arr[semester]  # variable in settings.py\n",
    "    time_breaks = ptq.split_text_progression_by_threshold(\n",
    "        df,\n",
    "        semester, \n",
    "        start_date=datetime(sft[0][0], sft[0][1], sft[0][2]), \n",
    "        end_date=datetime(sft[1][0], sft[1][1], sft[1][2]),\n",
    "        group_column='moodle_pad_id',\n",
    "        period_split_interval=period_split_interval\n",
    "        )\n",
    "    time_breaks.sort_values(by=['moodle_group_id', 'moodle_pad_id'])\n",
    "\n",
    "    print(f'## step3 - extract text versions in semester {the_semester[0]}') \n",
    "    es = Extract_Easy_Sync(semester, time_breaks=time_breaks, period_split_interval=period_split_interval)\n",
    "    es.is_reconstructing_text = True\n",
    "    print_all_output = True\n",
    "    pads = df.groupby('moodle_pad_id', group_keys=True)\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    i = 0\n",
    "    for index, pad in pads:\n",
    "        print(' ')\n",
    "        for row in pad.sort_values(by=['timestamp']).itertuples():\n",
    "            i = i + 1\n",
    "            if i > 0:\n",
    "                es.extract_changeset(\n",
    "                    row[5],             # row['textedit_changeset'], \n",
    "                    timestamp=row[6],   # row['timestamp'], \n",
    "                    group_id=row[3],    # row['moodle_group_id'], \n",
    "                    pad_id=row[4],      # str(row['moodle_pad_id'])\n",
    "                )\n",
    "        #print(es.ttext)\n",
    "    del pads, es\n",
    "    gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\"\"\"\n",
    "Compute text features from all text versions\n",
    "\"\"\"\n",
    "import gc  # Import garbage collector\n",
    "from src.settings import *\n",
    "from src.text_quality import Preprocess_Text_Quality\n",
    "\n",
    "semesters = [\n",
    "    # not available # 'SS2021', \n",
    "    ##('WS2021_22', 'dump20240826'),\n",
    "    ('SS2022',    'dump20240826'),\n",
    "    ##('WS2022_23', 'dump20240826'),\n",
    "    ##('SS2023',    'dump20240826'),\n",
    "    ##('WS2023_24',   'dump20240826'),\n",
    "    ##('SS2024',    'dump20240826'),\n",
    "    ##('WS2024_25',  'dump20241209'),\n",
    "]\n",
    "period_split_interval = 'weeks'\n",
    "min_index = 3500\n",
    "max_index = 5000\n",
    "for the_semester in semesters:\n",
    "    semester = the_semester[0]\n",
    "    ptq = Preprocess_Text_Quality(semester)    \n",
    "    print('## step4 - Compute text features') \n",
    "    prefix = 'edm25-' + semester\n",
    "    res_tq = ptq.determine_text_quality_from_files(\n",
    "        period_split_interval=period_split_interval, \n",
    "        prefix = prefix,\n",
    "        min_index = min_index,\n",
    "        max_index = max_index\n",
    "        )\n",
    "    del ptq.tq, ptq\n",
    "    gc.collect()\n",
    "res_tq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.text_quality import Text_Quality\n",
    "text = \"\"\"\n",
    "Niels Seidel (born October 7, 1981, in Löbau, Germany) is a German computer scientist, historian and academic known for his research in educational technology, learning analytics as well as Nazi concentration camps. He is a senior researcher at the Center of Advanced Technology for Assisted Learning and Predictive Analytics at FernUniversität in Hagen.\n",
    "\n",
    "==Early Life and Education==\n",
    "He grew up in Herrnhut, where he completed his school education from primary school to the Abitur. After a longer stay in Ireland he started his academic education in Media Informatics at Ulm Univeristy. In 2009 he obtained his degree as Diplom-Informatiker from there. He pursued his doctoral studies at the International Institute Zittau and later at the Technical University of Dresden, where he earned a Dr. rer. pol. (Ph.D.) in 2018.\n",
    "During his time in Ulm he wrote his first book about the concentration camps in Görlitz and Rennersdorf which was well recognised by historians and later awarded by the Saxon State. In Ulm Seidel was involved with freeFM, where he hosted a radio show for over two years. Additionally, he participated in numerous art projects in the fields of experimental music with Dieter Trüstedt and political street art. During his time in Zittau, he founded PiraCine, as the first non-profit cinema that exclusively screened films released under free licenses. In 20xx he presented Terezin Explained at the Neisse Filmfest.\n",
    "\n",
    "==Academic Career==\n",
    "From 2009 to 2017, Seidel worked as a research assistant at the Technical University of Dresden in the Production Management and Information Technology department. During this period, he contributed to developing CSCL (Computer-Supported Collaborative Learning) systems and conducted extensive research on interaction design patterns and hypervideo learning environments.\n",
    "\n",
    "Since 2017, Seidel has held the position of postdoctoral researcher at FernUniversität in Hagen. He has played a leading role in the development and evaluation of adaptive learning environments, with a particular focus on personalizing education through learning analytics and educational data mining.\n",
    "\n",
    "His research interests encompass adaptive learning environments, technology-enhanced learning, and diversity-inspired assessment. He has been involved in various high-profile projects, such as the LifeLong Learning Open Operating Platform (L³OOP) and APLE II. In addition to his technical leadership roles, Seidel has been recognized for his efforts in educational innovation, including the design and development of video-based learning platforms and tools for competency-based learning.\n",
    "Research and Projects\n",
    "\n",
    "==Awards and Recognition==\n",
    "Seidel’s work has been widely recognized, earning him several awards, including:\n",
    "* Winner of the first Saxon State Prize for Local History (Youth Promotion Award, 2008)\n",
    "* Young Scientist Award, Academic Coordination Center Euroregion Nisa (2012)\n",
    "* Scholarship in the E-Science Research Cluster Saxony (2012–2014)\n",
    "* Scholarship from the Multimedia Fund of the Technical University of Dresden (2014)\n",
    "* Winner of the Google Impact Challenge Germany (2018)\n",
    "\n",
    "==Publications (selection)==\n",
    "\n",
    "* Seidel, N. (2008). Die KZ-Außenlager Görlitz und Rennersdorf. Ein Beitrag zur Aufarbeitung der Geschehnisse im KZ Groß-Rosen. Neisse Verlag.\n",
    "\n",
    "* Claus, T., & Seidel, N. (2014). Werkstatt europäischen Denkens -- 20 Jahre Internationales Hochschulinstitut Zittau. TUDpress. http://nbn-resolving.de/urn:nbn:de:bsz:14-qucosa-152171\n",
    "\n",
    "* ...\n",
    "\n",
    "\n",
    "==External Links==\n",
    "\n",
    "\n",
    "==External Links==\n",
    "\n",
    "    ORCID\n",
    "    Google Scholar\n",
    "    ResearchGate\n",
    "    Literatur von Niels Seidel in der Deutschen Nationalbibliothek https://d-nb.info/gnd/1154866130\n",
    "\n",
    "\"\"\"\n",
    "text = \"hello world. excellent job.\"\n",
    "ptq = Text_Quality()  \n",
    "ptq.init_spacy(text)\n",
    "ptq.compute_duplicate_paragraphs(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example usage\n",
    "# Create a test directed graph\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from([\n",
    "    (1, 2, {'weight': 3}),\n",
    "    (2, 3, {'weight': 5}),\n",
    "    (3, 4, {'weight': 2}),\n",
    "    (4, 1, {'weight': 4}),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 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\n",
    "from huggingface_hub import notebook_login\n",
    "#notebook_login()\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import logging\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "# Example text\n",
    "text1 = \"Renewable energy is the future.\"\n",
    "text2 = \"However, it cannot fully replace fossil fuels due to reliability issues.\"\n",
    "text1 = \"Earth is not flat\"\n",
    "text2 = \"Der Hahn kräht\"\n",
    "text2 = \"Atomar power is not safe\"\n",
    "\n",
    "# Detect fake news\n",
    "model_name = \"openai-community/roberta-base-openai-detector\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "result = pipe(text1)\n",
    "print(result)\n",
    "\n",
    "# Relation classification model\n",
    "relation_pipeline = pipeline(\"text-classification\", model=\"raruidol/ArgumentMining-EN-ARI-AIF-RoBERTa_L\")\n",
    "result = relation_pipeline(f\"{text1} [SEP] {text2}\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!pip install textdescriptives\n",
    "#!spacy download en_core_web_sm\n",
    "import spacy\n",
    "import textdescriptives as td\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"textdescriptives/quality\")\n",
    "doc = nlp(\"The world is changed. I feel it in the \\n water. I feel it in\\n the earth. I smell it in the air. Much that once was is lost, for none now live who remember it.\")\n",
    "\n",
    "# all attributes are stored as a dict in the ._.quality attribute\n",
    "doc._.quality\n",
    "# check if the document passed all quality checks\n",
    "doc._.passed_quality_check\n",
    "\n",
    "# extract to dataframe\n",
    "td.extract_df(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
